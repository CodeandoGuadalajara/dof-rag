{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generador de Embeddings para Documentos del DOF\n",
    "\n",
    "## Índice de Contenidos\n",
    "- [1. Descripción General](#1)\n",
    "- [2. Características Principales](#2)\n",
    "- [3. Requisitos y Dependencias](#3)\n",
    "- [4. Instalación](#4)\n",
    "- [5. Estructura de Directorios](#5)\n",
    "- [6. Importación de Librerías](#6)\n",
    "- [7. Configuración de Modelos](#7)\n",
    "- [8. Inicialización de Base de Datos](#8)\n",
    "- [9. Funciones de Procesamiento](#9)\n",
    "- [10. Procesamiento de Archivos](#10)\n",
    "- [11. Widget para Procesamiento](#11)\n",
    "- [12. Procesamiento Manual](#12)\n",
    "- [13. Visor de Chunks](#13)\n",
    "- [14. Sistema de Consulta con Gemini](#14)\n",
    "- [15. Notas Importantes](#15)\n",
    "\n",
    "<h2 id=\"1\">1. Descripción General</h2>\n",
    "\n",
    "Este sistema procesa archivos markdown del Diario Oficial de la Federación (DOF), genera embeddings semánticos y permite realizar consultas utilizando búsqueda vectorial e inteligencia artificial. El sistema divide los documentos en secciones jerárquicas basadas en sus encabezados, fragmenta el texto y utiliza modelos de embedding para indexar el contenido.\n",
    "\n",
    "El notebook implementa:\n",
    "- Detección automática de encabezados en Markdown\n",
    "- División del texto en chunks suaves (sin romper oraciones)\n",
    "- Generación de encabezados contextuales para cada fragmento\n",
    "- Almacenamiento en base de datos SQLite con capacidades vectoriales\n",
    "- Consultas semánticas con Gemini 2.0\n",
    "\n",
    "<h2 id=\"2\">2. Características Principales</h2>\n",
    "\n",
    "- **Procesamiento Jerárquico**: Detecta automáticamente encabezados y subtítulos para mantener la estructura del documento.\n",
    "- **Fragmentación Suave**: Divide el texto respetando límites naturales como oraciones.\n",
    "- **Generación de Embeddings**: Utiliza modelos modernos para convertir texto a vectores semánticos.\n",
    "- **Almacenamiento Vectorial**: Base de datos SQLite optimizada para búsquedas vectoriales.\n",
    "- **Consultas con IA**: Integración con Gemini 2.0 para respuestas contextualizadas.\n",
    "- **Interfaz Interactiva**: Widgets interactivos para procesamiento y consultas.\n",
    "\n",
    "<h2 id=\"3\">3. Requisitos y Dependencias</h2>\n",
    "\n",
    "El sistema depende de las siguientes bibliotecas y tecnologías:\n",
    "\n",
    "- typer: para la interfaz de línea de comandos\n",
    "- fastlite: para manipulación simplificada de bases de datos SQLite\n",
    "- sqlite-vec: para habilitar capacidades vectoriales en SQLite\n",
    "- sentence-transformers: para la generación de embeddings\n",
    "- tokenizers: para tokenizar el texto\n",
    "- tqdm: para mostrar barras de progreso\n",
    "- google.generativeai: para integración con Gemini 2.0\n",
    "- numpy: para procesamiento numérico\n",
    "- python-dotenv: para cargar variables de entorno\n",
    "- ipywidgets: para widgets interactivos\n",
    "\n",
    "<h2 id=\"4\">4. Instalación</h2>\n",
    "\n",
    "1. Clona el repositorio o descarga los archivos\n",
    "2. Instala las dependencias necesarias usando uv (instalador de paquetes más rápido y moderno):\n",
    "\n",
    "```bash\n",
    "# Instalar uv si no lo tienes\n",
    "curl -sSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Instalar dependencias con uv\n",
    "uv pip install typer fastlite sqlite-vec sentence-transformers tokenizers tqdm google-generativeai numpy python-dotenv ipywidgets\n",
    "```\n",
    "\n",
    "3. Configura una API key de Google Gemini en un archivo .env:\n",
    "\n",
    "```\n",
    "GOOGLE_API_KEY=tu_api_key_aquí\n",
    "```\n",
    "\n",
    "<h2 id=\"5\">5. Estructura de Directorios</h2>\n",
    "\n",
    "```\n",
    "├── DOF_Embeddings_Generator.ipynb # Notebook principal\n",
    "├── dof_db/ # Directorio para la base de datos\n",
    "│   └── db.sqlite # Base de datos SQLite\n",
    "├── .env # Archivo de variables de entorno\n",
    "└── documentos_dof/ # Documentos markdown para procesar\n",
    "```\n",
    "\n",
    "<h2 id=\"6\">6. Importación de Librerías</h2>\n",
    "\n",
    "En esta sección se importan todas las bibliotecas necesarias para el funcionamiento del sistema. Incluye bibliotecas para:\n",
    "- Manipulación de archivos y expresiones regulares\n",
    "- Base de datos y vectores\n",
    "- Modelos de embeddings\n",
    "- Widgets interactivos\n",
    "- Integración con Gemini\n",
    "\n",
    "<h2 id=\"7\">7. Configuración de Modelos</h2>\n",
    "\n",
    "Configuración del modelo de embeddings y tokenizador:\n",
    "- Se utiliza \"nomic-ai/modernbert-embed-base\" para generar embeddings vectoriales de alta calidad\n",
    "- Se establece la longitud máxima de cada chunk (por defecto 1000 caracteres)\n",
    "\n",
    "<h2 id=\"8\">8. Inicialización de Base de Datos</h2>\n",
    "\n",
    "Esta sección configura la base de datos SQLite con extensiones vectoriales:\n",
    "- Crea/conecta a la base de datos en \"dof_db/db.sqlite\"\n",
    "- Habilita extensiones para búsqueda vectorial con sqlite-vec\n",
    "- Define esquema para tablas \"documents\" y \"chunks\"\n",
    "- Establece relaciones entre documentos y sus fragmentos\n",
    "\n",
    "<h2 id=\"9\">9. Funciones de Procesamiento</h2>\n",
    "\n",
    "Funciones clave para el procesamiento de documentos:\n",
    "\n",
    "1. **parse_text_by_headings**: Divide el texto en secciones basadas en la jerarquía de encabezados\n",
    "2. **split_text_smooth**: Divide el texto en chunks respetando límites de oraciones\n",
    "3. **build_chunk_header**: Genera encabezados contextuales para cada fragmento\n",
    "4. **get_url_from_filename**: Obtiene la URL oficial del DOF a partir del nombre del archivo\n",
    "\n",
    "<h2 id=\"10\">10. Procesamiento de Archivos</h2>\n",
    "\n",
    "Estas funciones manejan el procesamiento completo de archivos y directorios:\n",
    "\n",
    "1. **process_file**: Procesa un archivo markdown individual\n",
    "   - Extrae metadatos del documento\n",
    "   - Parsea la estructura jerárquica\n",
    "   - Divide el texto en chunks\n",
    "   - Genera embeddings\n",
    "   - Almacena todo en la base de datos\n",
    "   - Crea un archivo de depuración\n",
    "\n",
    "2. **process_directory**: Procesa todos los archivos markdown en un directorio\n",
    "\n",
    "<h2 id=\"11\">11. Widget para Procesamiento</h2>\n",
    "\n",
    "Interfaz interactiva para procesar documentos del DOF. Pasos para usarlo:\n",
    "\n",
    "1. Ingresa la ruta al directorio que contiene los archivos markdown del DOF\n",
    "2. Haz clic en \"Procesar archivos\" para iniciar el procesamiento\n",
    "3. Espera a que se complete el proceso y revisa los resultados\n",
    "\n",
    "<h2 id=\"12\">12. Procesamiento Manual (opcional)</h2>\n",
    "\n",
    "Esta sección permite ejecutar el procesamiento programáticamente sin usar los widgets.\n",
    "\n",
    "<h2 id=\"13\">13. Visor de Chunks Generados</h2>\n",
    "\n",
    "Interfaz para visualizar y explorar los chunks generados. Pasos para usarlo:\n",
    "\n",
    "1. Haz clic en \"Buscar archivos\" para encontrar todos los archivos de chunks generados\n",
    "2. Selecciona un archivo del menú desplegable\n",
    "3. Haz clic en \"Ver contenido\" para mostrar los chunks y sus encabezados\n",
    "\n",
    "<h2 id=\"14\">14. Sistema de Consulta con Gemini 2.0</h2>\n",
    "\n",
    "Esta sección implementa el sistema de búsqueda semántica y consulta con IA. Proceso de consulta:\n",
    "\n",
    "1. Ingresa una pregunta sobre los documentos del DOF\n",
    "2. El sistema busca los chunks más relevantes mediante similitud de embeddings\n",
    "3. Se obtiene contexto extendido (chunks adyacentes) para mejorar la comprensión\n",
    "4. Se formula un prompt para Gemini 2.0 con el contexto relevante\n",
    "5. Gemini 2.0 genera una respuesta contextualizada\n",
    "6. Se muestra la respuesta junto con información sobre la fuente\n",
    "\n",
    "<h2 id=\"15\">15. Notas Importantes</h2>\n",
    "\n",
    "- Los nombres de archivo deben seguir el formato `DDMMYYYY-XXX.md` para generar URLs correctas.\n",
    "- Para obtener mejores resultados, asegúrate de que los documentos tengan una estructura clara con encabezados markdown (# Título, ## Subtítulo, etc.).\n",
    "- La calidad de las respuestas depende de la riqueza del contenido indexado.\n",
    "- El tamaño máximo de chunk (1000 caracteres) podría necesitar ajustes según el modelo de embeddings.\n",
    "- La visualización de resultados está optimizada para notebooks Jupyter.\n",
    "- El rendimiento puede verse afectado cuando la base de datos contiene muchos documentos.\n",
    "\n",
    "[Volver al Índice](#Índice-de-Contenidos) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "import typer\n",
    "from fastlite import database\n",
    "from sqlite_vec import load, serialize_float32\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tokenizers import Tokenizer\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from os import getenv\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Importar widgets para la interfaz interactiva\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuración de modelos y tokenizadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de modelos y tokenizadores\n",
    "tokenizer = Tokenizer.from_pretrained(\"nomic-ai/modernbert-embed-base\")\n",
    "model = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", trust_remote_code=True)\n",
    "\n",
    "# Configuración: máximo de caracteres para cada chunk suave\n",
    "MAX_CHUNK_LENGTH = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inicialización de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Table chunks (id, document_id, text, header, embedding, created_at)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inicializar la base de datos con sqlite-vec\n",
    "db = database(\"dof_db/db.sqlite\")\n",
    "db.conn.enable_load_extension(True)\n",
    "load(db.conn)\n",
    "db.conn.enable_load_extension(False)\n",
    "\n",
    "# Crear/actualizar esquema de tablas\n",
    "db.t.documents.create(\n",
    "    id=int, \n",
    "    title=str, \n",
    "    url=str, \n",
    "    file_path=str, \n",
    "    created_at=datetime, \n",
    "    pk=\"id\", \n",
    "    ignore=True\n",
    ")\n",
    "db.t.documents.create_index([\"url\"], unique=True, if_not_exists=True)\n",
    "\n",
    "# Se añade la columna 'header' para guardar el encabezado contextual\n",
    "db.t.chunks.create(\n",
    "    id=int,\n",
    "    document_id=int,\n",
    "    text=str,\n",
    "    header=str,\n",
    "    embedding=bytes,\n",
    "    created_at=datetime,\n",
    "    pk=\"id\",\n",
    "    foreign_keys=[(\"document_id\", \"documents\")],\n",
    "    ignore=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para procesar el documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex para detectar headings en Markdown\n",
    "HEADING_PATTERN = re.compile(r'^(#{1,6})\\s+(.*)$')\n",
    "\n",
    "def parse_text_by_headings(text: str):\n",
    "    \"\"\"\n",
    "    Divide el texto en secciones basadas en los headings de Markdown.\n",
    "    Retorna una lista de diccionarios con:\n",
    "      - \"heading_hierarchy\": lista con la jerarquía de headings\n",
    "      - \"content\": contenido de la sección\n",
    "    \"\"\"\n",
    "    lines = text.split('\\n')\n",
    "    sections = []\n",
    "    current_hierarchy = []\n",
    "    current_content_lines = []\n",
    "\n",
    "    def add_section(hierarchy, content_lines):\n",
    "        if not hierarchy or not content_lines:\n",
    "            return\n",
    "        sections.append({\n",
    "            \"heading_hierarchy\": hierarchy.copy(),\n",
    "            \"content\": \"\\n\".join(content_lines).strip()\n",
    "        })\n",
    "\n",
    "    for line in lines:\n",
    "        heading_match = HEADING_PATTERN.match(line)\n",
    "        if heading_match:\n",
    "            # Cuando se detecta un heading, se cierra la sección anterior\n",
    "            add_section(current_hierarchy, current_content_lines)\n",
    "            current_content_lines = []\n",
    "            hashes = heading_match.group(1)\n",
    "            heading_text = heading_match.group(2).strip()\n",
    "            level = len(hashes)\n",
    "            # Ajustar la jerarquía: mantener los niveles anteriores hasta (nivel-1)\n",
    "            current_hierarchy = current_hierarchy[:level-1]\n",
    "            current_hierarchy.append(heading_text)\n",
    "        else:\n",
    "            current_content_lines.append(line)\n",
    "\n",
    "    # Agregar la última sección si existe contenido\n",
    "    add_section(current_hierarchy, current_content_lines)\n",
    "    return sections\n",
    "\n",
    "def split_text_smooth(text: str, max_length: int = MAX_CHUNK_LENGTH, min_chunk_ratio: float = 0.5):\n",
    "    \"\"\"\n",
    "    Divide el texto en chunks suaves sin romper oraciones.\n",
    "    Separa el texto en oraciones basándose en signos de puntuación y las agrupa sin\n",
    "    exceder el límite de caracteres (max_length). Si el último chunk es muy corto\n",
    "    (menos de min_chunk_ratio * max_length), se fusiona con el chunk anterior.\n",
    "    \"\"\"\n",
    "    # Separa el texto en oraciones (manteniendo el signo de puntuación)\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) + 1 <= max_length:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        current_chunk = current_chunk.strip()\n",
    "        # Si el último chunk es muy corto y existe un chunk previo, fusiónalo\n",
    "        if chunks and len(current_chunk) < max_length * min_chunk_ratio:\n",
    "            previous_chunk = chunks.pop()\n",
    "            merged = previous_chunk + \" \" + current_chunk\n",
    "            chunks.append(merged.strip())\n",
    "        else:\n",
    "            chunks.append(current_chunk)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def build_chunk_header(doc_title: str, heading_hierarchy: list):\n",
    "    \"\"\"\n",
    "    Construye el encabezado contextual utilizando el título del documento y la jerarquía de headings.\n",
    "    \"\"\"\n",
    "    if not heading_hierarchy:\n",
    "        return f\"Document: {doc_title}\"\n",
    "    hierarchy_str = \" > \".join(heading_hierarchy)\n",
    "    return f\"Document: {doc_title} | Section: {hierarchy_str}\"\n",
    "\n",
    "def get_url_from_filename(filename: str) -> str:\n",
    "    \"\"\"\n",
    "    Genera la URL con base en el patrón del nombre de archivo.\n",
    "    Formato esperado: DDMMYYYY-XXX.md\n",
    "    Ejemplo: 23012025-MAT.md\n",
    "    \"\"\"\n",
    "    base_filename = os.path.basename(filename).replace(\".md\", \"\")\n",
    "    if len(base_filename) >= 8:\n",
    "        year = base_filename[4:8]\n",
    "        pdf_filename = f\"{base_filename}.pdf\"\n",
    "        url = f\"https://diariooficial.gob.mx/abrirPDF.php?archivo={pdf_filename}&anio={year}&repo=repositorio/\"\n",
    "        return url\n",
    "    else:\n",
    "        raise ValueError(f\"Expected filename like 23012025-MAT.md but got {filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones para procesamiento de archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_file(file_path: str):\n",
    "    \"\"\"\n",
    "    Procesa un archivo markdown, extrae secciones basadas en headings,\n",
    "    divide cada sección en chunks suaves, genera embeddings y los almacena en la base de datos.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        content = file.read()\n",
    "\n",
    "    # 1) Extraer metadatos y preparar la inserción del documento\n",
    "    title = os.path.splitext(os.path.basename(file_path))[0]\n",
    "    url = get_url_from_filename(file_path)\n",
    "\n",
    "    # Eliminar cualquier versión anterior del documento\n",
    "    db.t.documents.delete_where(\"url = ?\", [url])\n",
    "    doc = db.t.documents.insert(\n",
    "        title=title, \n",
    "        url=url, \n",
    "        file_path=file_path, \n",
    "        created_at=datetime.now()\n",
    "    )\n",
    "\n",
    "    # 2) Parsear el contenido basado en headings\n",
    "    sections = parse_text_by_headings(content)\n",
    "\n",
    "    # Archivo de salida para depuración de chunks\n",
    "    chunks_file_path = os.path.splitext(file_path)[0] + \"_chunks.txt\"\n",
    "    # Definimos un contador global para numerar secuencialmente los chunks\n",
    "    global_chunk_counter = 1\n",
    "    with open(chunks_file_path, \"w\", encoding=\"utf-8\") as chunks_file:\n",
    "        # 3) Procesar cada sección\n",
    "        for section in sections:\n",
    "            hierarchy = section.get(\"heading_hierarchy\", [])\n",
    "            section_content = section.get(\"content\", \"\")\n",
    "            # Construir el encabezado contextual\n",
    "            header = build_chunk_header(title, hierarchy)\n",
    "            # Dividir la sección en chunks suaves\n",
    "            sub_chunks = split_text_smooth(section_content, max_length=MAX_CHUNK_LENGTH)\n",
    "\n",
    "            for i, chunk in enumerate(sub_chunks):\n",
    "                # Texto completo para el embedding: encabezado + chunk\n",
    "                text_for_embedding = f\"{header}\\n\\n{chunk}\"\n",
    "                embedding = model.encode(f\"search_document: {text_for_embedding}\")\n",
    "\n",
    "                # Guardar en el archivo de depuración\n",
    "                chunks_file.write(f\"--- CHUNK #{global_chunk_counter} ---\\n\")\n",
    "                chunks_file.write(f\"Header: {header}\\n\")\n",
    "                chunks_file.write(f\"Texto:\\n{chunk}\\n\")\n",
    "                chunks_file.write(\"\\n\" + \"-\"*50 + \"\\n\\n\")\n",
    "                global_chunk_counter += 1\n",
    "\n",
    "                # Insertar en la base de datos\n",
    "                db.t.chunks.insert(\n",
    "                    document_id=doc[\"id\"],\n",
    "                    text=chunk,\n",
    "                    header=header,\n",
    "                    embedding=embedding,\n",
    "                    created_at=datetime.now(),\n",
    "                )\n",
    "\n",
    "    return f\"✅ Procesado completado para: {file_path}\\nSe generó el archivo de chunks en: {chunks_file_path}\"\n",
    "\n",
    "def process_directory(directory: str):\n",
    "    \"\"\"Procesa todos los archivos .md de un directorio.\"\"\"\n",
    "    results = []\n",
    "    md_files = [f for f in os.listdir(directory) if f.endswith(\".md\")]\n",
    "    \n",
    "    if not md_files:\n",
    "        return \"⚠️ No se encontraron archivos .md en el directorio especificado.\"\n",
    "    \n",
    "    for f in tqdm(md_files, desc=\"Procesando archivos\"):\n",
    "        file_path = os.path.join(directory, f)\n",
    "        result = process_file(file_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    return \"\\n\\n\".join(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Widget para ingresar la ruta de procesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b96112319244b548a2f1ec51ccfb04d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Procesamiento de archivos DOF</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7d15aef305b4fb189ee4310ecaac837",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', description='Directorio:', layout=Layout(width='80%'), placeholder='Ingresa la ruta del directo…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef234ca8c80341408a31b47c3e399f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Procesar archivos', icon='check', style=ButtonStyle(), tooltip='Ha…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b11b105be644d7bb98118b7acffc650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Crear widget de entrada para la ruta del directorio\n",
    "text_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ingresa la ruta del directorio con archivos .md',\n",
    "    description='Directorio:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "process_button = widgets.Button(\n",
    "    description='Procesar archivos',\n",
    "    button_style='primary',\n",
    "    tooltip='Haz clic para procesar los archivos en el directorio especificado',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        directory = text_input.value.strip()\n",
    "    md_files = [f for f in os.listdir(directory) if f.endswith(\".md\")]\n",
    "    \n",
    "    if not md_files:\n",
    "        return \"⚠️ No se encontraron archivos .md en el directorio especificado.\"\n",
    "    \n",
    "    for f in tqdm(md_files, desc=\"Procesando archivos\"):\n",
    "        file_path = os.path.join(directory, f)\n",
    "        result = process_file(file_path)\n",
    "        results.append(result)\n",
    "    \n",
    "    return \"\\n\\n\".join(results)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        directory = text_input.value.strip()\n",
    "        \n",
    "        if not directory:\n",
    "            print(\"⚠️ Por favor ingresa una ruta de directorio válida.\")\n",
    "            return\n",
    "        \n",
    "        if not os.path.exists(directory):\n",
    "            print(f\"⚠️ El directorio '{directory}' no existe.\")\n",
    "            return\n",
    "            \n",
    "        print(f\"🔄 Procesando archivos en: {directory}\")\n",
    "        try:\n",
    "            result = process_directory(directory)\n",
    "            print(result)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error durante el procesamiento: {str(e)}\")\n",
    "\n",
    "process_button.on_click(on_button_clicked)\n",
    "\n",
    "# Mostrar los widgets\n",
    "display(widgets.HTML(\"<h3>Procesamiento de archivos DOF</h3>\"))\n",
    "display(text_input)\n",
    "display(process_button)\n",
    "display(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo de procesamiento manual (opcional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de cómo procesar un directorio específico manualmente\n",
    "# Descomenta la siguiente línea y especifica la ruta\n",
    "# result = process_directory(\"ruta/a/tu/directorio\")\n",
    "# print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visor de Chunks Generados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30889b364c014386bcf4990050efeff8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Visor de Chunks Generados</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "829fc801649d400fbe5e203dd2d46bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Buscar archivos', icon='refresh', style=ButtonStyle(),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6232e21051ef4e969ed5138d5e1af794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='success', description='Ver contenido', icon='eye', style=ButtonStyle(), tooltip='Mostrar …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed46f29dd1b4b5894ae521fa7d41247",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border_bottom='1px solid #ddd', border_left='1px solid #ddd', border_right='1px solid #dd…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import glob\n",
    "import os\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Widget para seleccionar archivo de chunks\n",
    "file_selector = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description='Archivo:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "refresh_button = widgets.Button(\n",
    "    description='Buscar archivos',\n",
    "    button_style='info',\n",
    "    tooltip='Actualizar lista de archivos de chunks',\n",
    "    icon='refresh'\n",
    ")\n",
    "\n",
    "view_button = widgets.Button(\n",
    "    description='Ver contenido',\n",
    "    button_style='success',\n",
    "    tooltip='Mostrar el contenido del archivo seleccionado',\n",
    "    icon='eye'\n",
    ")\n",
    "\n",
    "chunk_output = widgets.Output(layout=widgets.Layout(\n",
    "    border='1px solid #ddd',\n",
    "    max_height='500px',\n",
    "    overflow_y='auto',\n",
    "    padding='10px'\n",
    "))\n",
    "\n",
    "def update_file_list(b=None):\n",
    "    # Buscar todos los archivos _chunks.txt en el directorio de la entrada de texto\n",
    "    directory = text_input.value.strip() if hasattr(text_input, 'value') and text_input.value.strip() else '.'\n",
    "    chunk_files = glob.glob(os.path.join(directory, \"*_chunks.txt\"))\n",
    "    \n",
    "    # Añadir también la búsqueda recursiva en subdirectorios\n",
    "    if not chunk_files:\n",
    "        for root, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                if file.endswith(\"_chunks.txt\"):\n",
    "                    chunk_files.append(os.path.join(root, file))\n",
    "    \n",
    "    # Actualizar opciones del selector\n",
    "    file_selector.options = [os.path.basename(f) for f in sorted(chunk_files)]\n",
    "    file_selector.paths = {os.path.basename(f): f for f in sorted(chunk_files)}\n",
    "    \n",
    "    with chunk_output:\n",
    "        chunk_output.clear_output()\n",
    "        if not chunk_files:\n",
    "            print(f\"❌ No se encontraron archivos de chunks en '{directory}' ni sus subdirectorios.\")\n",
    "        else:\n",
    "            print(f\"✅ Se encontraron {len(chunk_files)} archivos de chunks.\")\n",
    "\n",
    "def display_chunk_file(b):\n",
    "    with chunk_output:\n",
    "        chunk_output.clear_output()\n",
    "        if not file_selector.options:\n",
    "            print(\"❌ No hay archivos para mostrar. Usa 'Buscar archivos' primero.\")\n",
    "            return\n",
    "            \n",
    "        selected_file = file_selector.value\n",
    "        full_path = file_selector.paths[selected_file]\n",
    "        \n",
    "        try:\n",
    "            with open(full_path, 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Formatear el contenido para HTML\n",
    "            content_html = content.replace('\\n', '<br>')\n",
    "            content_html = re.sub(r'--- CHUNK #(\\d+) ---', r'<h4 style=\"color:blue;\">--- CHUNK #\\1 ---</h4>', content_html)\n",
    "            content_html = re.sub(r'Header: (.*?)<br>', r'<b>Header:</b> <span style=\"color:green;\">\\1</span><br>', content_html)\n",
    "            content_html = re.sub(r'-{50}', r'<hr style=\"border-top: 1px dashed #ccc;\">', content_html)\n",
    "            \n",
    "            display(HTML(f\"<h3>Contenido de: {selected_file}</h3>\"))\n",
    "            display(HTML(content_html))\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error al leer el archivo: {str(e)}\")\n",
    "\n",
    "refresh_button.on_click(update_file_list)\n",
    "view_button.on_click(display_chunk_file)\n",
    "\n",
    "# Mostrar widgets\n",
    "display(widgets.HTML(\"<h3>Visor de Chunks Generados</h3>\"))\n",
    "display(widgets.HBox([refresh_button, file_selector]))\n",
    "display(view_button)\n",
    "display(chunk_output)\n",
    "\n",
    "# Inicializar la lista de archivos\n",
    "update_file_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sistema de Consulta con Gemini 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a99b71724857479eb3b08e2a6c0f37c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='<h3>Consulta de Documentos DOF con Gemini 2.0</h3>')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6813b1e0e03e4c058dc0b302488335cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Ingresa una consulta sobre los documentos del Diario Oficial de la Federación:'), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import google.generativeai as genai\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno y configurar Gemini\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "if not api_key:\n",
    "    print(\"⚠️ GOOGLE_API_KEY no está configurada en las variables de entorno\")\n",
    "else:\n",
    "    genai.configure(api_key=api_key)\n",
    "\n",
    "# Usar el mismo modelo de embeddings que usamos para indexar\n",
    "model_query = SentenceTransformer(\"nomic-ai/modernbert-embed-base\", trust_remote_code=True)\n",
    "\n",
    "# Funciones para procesamiento de consultas\n",
    "\n",
    "def deserialize_embedding(blob):\n",
    "    \"\"\"Convierte un BLOB almacenado en la base de datos a un vector NumPy de tipo float32.\"\"\"\n",
    "    return np.frombuffer(blob, dtype=np.float32)\n",
    "\n",
    "def get_all_chunks():\n",
    "    \"\"\"Obtiene todos los chunks almacenados en la base de datos.\"\"\"\n",
    "    results = db.query(\n",
    "        \"\"\"\n",
    "        SELECT \n",
    "            c.id as chunk_id,\n",
    "            c.text as chunk_text,\n",
    "            c.header as chunk_header,\n",
    "            c.document_id as document_id,\n",
    "            d.title as doc_title,\n",
    "            d.url as doc_url,\n",
    "            c.embedding as embedding_blob\n",
    "        FROM chunks c\n",
    "        JOIN documents d ON c.document_id = d.id\n",
    "        ORDER BY c.id;\n",
    "        \"\"\"\n",
    "    )\n",
    "    return list(results)\n",
    "\n",
    "def find_relevant_chunks(query, all_chunks, top_k=10):\n",
    "    \"\"\"Encuentra los chunks más relevantes para la consulta.\"\"\"\n",
    "    # Generar embedding para la consulta\n",
    "    query_embedding = model_query.encode(f\"search_query: {query}\")\n",
    "    \n",
    "    # Calcular distancias\n",
    "    scored_results = []\n",
    "    for chunk in all_chunks:\n",
    "        chunk_embedding = deserialize_embedding(chunk[\"embedding_blob\"])\n",
    "        distance = np.linalg.norm(query_embedding - chunk_embedding)\n",
    "        scored_results.append({\n",
    "            \"chunk_id\": chunk[\"chunk_id\"],\n",
    "            \"chunk_text\": chunk[\"chunk_text\"],\n",
    "            \"chunk_header\": chunk[\"chunk_header\"],\n",
    "            \"document_id\": chunk[\"document_id\"],\n",
    "            \"doc_title\": chunk[\"doc_title\"],\n",
    "            \"doc_url\": chunk[\"doc_url\"],\n",
    "            \"distance\": distance,\n",
    "            \"similarity\": 1.0 / (1.0 + distance)  # Convertir distancia a similitud\n",
    "        })\n",
    "    \n",
    "    # Ordenar por menor distancia (mayor similitud)\n",
    "    return sorted(scored_results, key=lambda x: x[\"distance\"])[:top_k]\n",
    "\n",
    "def get_extended_context(best_chunk, all_chunks):\n",
    "    \"\"\"Obtiene un contexto extendido incluyendo chunks adyacentes.\"\"\"\n",
    "    # Filtrar chunks del mismo documento\n",
    "    same_doc_chunks = [c for c in all_chunks if c[\"document_id\"] == best_chunk[\"document_id\"]]\n",
    "    \n",
    "    # Ordenar por ID para mantener el orden original\n",
    "    same_doc_chunks = sorted(same_doc_chunks, key=lambda c: c[\"chunk_id\"])\n",
    "    \n",
    "    # Encontrar la posición del mejor chunk\n",
    "    best_index = None\n",
    "    for i, chunk in enumerate(same_doc_chunks):\n",
    "        if chunk[\"chunk_id\"] == best_chunk[\"chunk_id\"]:\n",
    "            best_index = i\n",
    "            break\n",
    "    \n",
    "    # Construir contexto extendido\n",
    "    context_parts = []\n",
    "    context_headers = []\n",
    "    \n",
    "    if best_index is not None:\n",
    "        # Añadir chunk anterior si existe\n",
    "        if best_index > 0:\n",
    "            context_parts.append(same_doc_chunks[best_index - 1][\"chunk_text\"])\n",
    "            context_headers.append(same_doc_chunks[best_index - 1][\"chunk_header\"])\n",
    "            \n",
    "        # Añadir el mejor chunk\n",
    "        context_parts.append(best_chunk[\"chunk_text\"])\n",
    "        context_headers.append(best_chunk[\"chunk_header\"])\n",
    "        \n",
    "        # Añadir chunk siguiente si existe\n",
    "        if best_index < len(same_doc_chunks) - 1:\n",
    "            context_parts.append(same_doc_chunks[best_index + 1][\"chunk_text\"])\n",
    "            context_headers.append(same_doc_chunks[best_index + 1][\"chunk_header\"])\n",
    "    else:\n",
    "        # Si no se encuentra (caso raro), usar solo el mejor chunk\n",
    "        context_parts.append(best_chunk[\"chunk_text\"])\n",
    "        context_headers.append(best_chunk[\"chunk_header\"])\n",
    "    \n",
    "    # Generar contexto con headers\n",
    "    full_context = \"\"\n",
    "    for i, (header, text) in enumerate(zip(context_headers, context_parts)):\n",
    "        full_context += f\"[Sección {i+1}]: {header}\\n\\n{text}\\n\\n\"\n",
    "    \n",
    "    return full_context\n",
    "\n",
    "def query_gemini(query, query_output):\n",
    "    \"\"\"Realiza una consulta completa: búsqueda de chunks y generación de respuesta con Gemini.\"\"\"\n",
    "    try:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(f\"🔍 Procesando consulta: '{query}'\")\n",
    "            print(\"Obteniendo chunks de la base de datos...\")\n",
    "        \n",
    "            # Obtener todos los chunks\n",
    "            all_chunks = get_all_chunks()\n",
    "            \n",
    "            if not all_chunks:\n",
    "                print(\"❌ No se encontraron documentos en la base de datos.\")\n",
    "                return\n",
    "                \n",
    "            print(f\"Se obtuvieron {len(all_chunks)} chunks totales.\")\n",
    "            print(\"Buscando chunks relevantes...\")\n",
    "            \n",
    "            # Encontrar chunks relevantes\n",
    "            top_results = find_relevant_chunks(query, all_chunks)\n",
    "            \n",
    "            if not top_results:\n",
    "                print(\"❌ No se encontraron chunks relevantes para esta consulta.\")\n",
    "                return\n",
    "                \n",
    "            # Obtener el mejor resultado\n",
    "            best_chunk = top_results[0]\n",
    "            \n",
    "            print(f\"Se encontraron {len(top_results)} chunks relevantes.\")\n",
    "            print(f\"Mejor chunk: ID={best_chunk['chunk_id']}, Distancia={best_chunk['distance']:.4f}\")\n",
    "            print(\"Obteniendo contexto extendido...\")\n",
    "            \n",
    "            # Obtener contexto extendido (con chunks adyacentes)\n",
    "            extended_context = get_extended_context(best_chunk, all_chunks)\n",
    "            \n",
    "            print(\"Generando prompt para Gemini...\")\n",
    "            \n",
    "            # Construir prompt para Gemini\n",
    "            prompt = (\n",
    "                f\"Utilizando el siguiente contexto extraído de un documento del Diario Oficial de la Federación:\\n\\n\"\n",
    "                f\"---\\n{extended_context}\\n---\\n\\n\"\n",
    "                f\"Instrucciones:\\n\"\n",
    "                f\"- Si la consulta está relacionada directamente con el contenido, responde basándote en la información proporcionada.\\n\"\n",
    "                f\"- Si la consulta es un saludo, despedida o una interacción social común, responde de forma cordial y adecuada.\\n\"\n",
    "                f\"- Si se solicita información que no se encuentra en el contexto, indica de manera clara que no hay información relevante disponible en el documento.\\n\"\n",
    "                f\"- Asegúrate de interpretar correctamente la solicitud, contestando únicamente lo que se pregunta y evitando agregar información irrelevante.\\n\\n\"\n",
    "                f\"Pregunta:\\n{query}\"\n",
    "            )\n",
    "            \n",
    "            print(\"Enviando consulta a Gemini...\")\n",
    "            \n",
    "            # Generar respuesta con Gemini\n",
    "            model_gemini = genai.GenerativeModel('gemini-2.0-flash')\n",
    "            response = model_gemini.generate_content(prompt)\n",
    "            \n",
    "            # Mostrar respuesta\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(f\"RESPUESTA A: '{query}'\")\n",
    "            print(\"=\"*60)\n",
    "            display(HTML(f\"<div style='background-color: #f8f9fa; padding: 15px; border-radius: 5px; margin: 10px 0;'>{response.text.replace(chr(10), '<br>')}</div>\"))\n",
    "            \n",
    "            print(\"\\n\" + \"-\"*60)\n",
    "            print(f\"Fuente: {best_chunk['doc_title']}\")\n",
    "            print(f\"URL: {best_chunk['doc_url']}\")\n",
    "            print(f\"Relevancia (distancia): {best_chunk['distance']:.4f}\")\n",
    "            print(f\"Similitud: {best_chunk['similarity']:.4f}\")\n",
    "            print(\"-\"*60)\n",
    "            \n",
    "            # Mostrar contexto utilizado en un formato colapsable\n",
    "            context_html = extended_context.replace('\\n', '<br>').replace('[Sección', '<b>[Sección')\n",
    "            context_html = context_html.replace(']:', ']:</b>')\n",
    "            \n",
    "            display(HTML(f\"\"\"\n",
    "            <details>\n",
    "                <summary style=\"cursor: pointer; color: #0066cc;\">Ver contexto utilizado (click para expandir)</summary>\n",
    "                <div style=\"background-color: #f5f5f5; padding: 10px; border: 1px solid #ddd; margin-top: 10px;\">\n",
    "                    {context_html}\n",
    "                </div>\n",
    "            </details>\n",
    "            \"\"\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        with query_output:\n",
    "            print(f\"❌ Error durante el proceso: {str(e)}\")\n",
    "\n",
    "# Crear widgets para la interfaz de consulta\n",
    "query_input = widgets.Text(\n",
    "    value='',\n",
    "    placeholder='Ingresa tu consulta sobre documentos del DOF',\n",
    "    description='Consulta:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "query_button = widgets.Button(\n",
    "    description='Consultar',\n",
    "    button_style='primary',\n",
    "    tooltip='Enviar consulta a Gemini',\n",
    "    icon='search'\n",
    ")\n",
    "\n",
    "query_output = widgets.Output(\n",
    "    layout=widgets.Layout(\n",
    "        border='1px solid #ddd',\n",
    "        padding='10px',\n",
    "        overflow_y='auto',\n",
    "        max_height='600px'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Función para manejar el evento del botón\n",
    "def on_query_button_clicked(b):\n",
    "    query = query_input.value.strip()\n",
    "    if not query:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(\"⚠️ Por favor ingresa una consulta válida.\")\n",
    "        return\n",
    "    \n",
    "    query_gemini(query, query_output)\n",
    "\n",
    "# Función para manejar el evento de tecla Enter\n",
    "def on_query_input_submit(sender):\n",
    "    query = query_input.value.strip()\n",
    "    if not query:\n",
    "        with query_output:\n",
    "            clear_output()\n",
    "            print(\"⚠️ Por favor ingresa una consulta válida.\")\n",
    "        return\n",
    "    \n",
    "    query_gemini(query, query_output)\n",
    "\n",
    "def on_query_input_changed(change):\n",
    "    if change.new and not change.old:  # Solo se activa cuando cambia de vacío a un valor\n",
    "        query = change.new.strip()\n",
    "        if query:\n",
    "            query_gemini(query, query_output)\n",
    "\n",
    "query_button.on_click(on_query_button_clicked)\n",
    "query_input.continuous_update = False  \n",
    "# Observar cambios en el valor en lugar de usar on_submit\n",
    "query_input.observe(lambda change: \n",
    "    on_query_gemini_if_enter(change, query_output), \n",
    "    names='value')\n",
    "\n",
    "def on_query_gemini_if_enter(change, output):\n",
    "    # Comprobar si se presionó Enter (el cambio viene con un valor nuevo)\n",
    "    if hasattr(change, 'new') and change.new.strip():\n",
    "        query = change.new.strip()\n",
    "        query_gemini(query, output)\n",
    "\n",
    "# Mostrar la interfaz\n",
    "display(widgets.HTML(\"<h3>Consulta de Documentos DOF con Gemini 2.0</h3>\"))\n",
    "display(widgets.VBox([\n",
    "    widgets.Label(\"Ingresa una consulta sobre los documentos del Diario Oficial de la Federación:\"),\n",
    "    widgets.HBox([query_input, query_button]),\n",
    "    query_output\n",
    "]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
