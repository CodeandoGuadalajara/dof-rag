{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"python-dotenv\",\n",
    "#   \"google>=0.3.0\",\n",
    "#   \"google-genai>=1.3.0\",\n",
    "#   \"pillow\",\n",
    "#   \"tqdm\",\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from modules.file_utils import FileUtil\n",
    "from modules.ui_components import create_processing_interface\n",
    "from clients.gemini_client import GeminiClient\n",
    "from clients.openai_client import OpenAIClient\n",
    "from clients.ollama_client import OllamaClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Resume brevemente la imagen en español (máximo 3-4 oraciones por categoría):  \n",
    "            - **Texto:** Menciona solo el título y 2-3 puntos clave si hay texto.\n",
    "            - **Mapas:** Identifica la región principal y máximo 2-3 ubicaciones relevantes.\n",
    "            - **Diagramas:** Resume el concepto central en 1-2 oraciones.\n",
    "            - **Logos:** Identifica la entidad y sus características distintivas.\n",
    "            - **Datos visuales:** Menciona solo los 2-3 valores o tendencias más importantes.\n",
    "            Prioriza la información esencial sobre los detalles, manteniendo la descripción breve y directa.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = GeminiClient(\n",
    "    model=\"gemini-2.0-flash\",    # Gemini model to use\n",
    "    max_tokens=256,             # Maximum number of tokens in the output\n",
    "    temperature=0.6,            # Controls the creativity of the generation\n",
    "    top_p=0.6,                  # Top_p value for generation\n",
    "    top_k=20,                   # Top_k value for generation\n",
    "    response_mime_type=\"text/plain\",  # MIME type of the response\n",
    "    api_key=os.getenv(\"GEMINI_API_KEY\")  # API key (optional, can be configured later)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAIClient(\n",
    "    model=\"gpt-4o\",                      # OpenAI model to use\n",
    "    max_tokens=512,                      # Maximum number of tokens in the output\n",
    "    temperature=0.6,                     # Controls the creativity of the generation\n",
    "    top_p=0.6,                           # Top_p value for generation\n",
    "    api_key=os.getenv(\"OPENAI_API_KEY\")  # API key (optional, can be configured later)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OllamaClient(\n",
    "    model=\"gemma3:4b\",          # Ollama model to use\n",
    "    max_tokens=512,             # Maximum number of tokens in the output\n",
    "    temperature=0.5,            # Controls the creativity of the generation\n",
    "    top_p=0.5,                  # Top_p value for generation\n",
    "    top_k=20,                   # Top_k value for generation\n",
    "    num_ctx=8192,               # Context size for the model\n",
    "    api_key=None                # Not used for Ollama, but kept for compatibility\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FileUtil instance with default values (can be modified from the interface)\n",
    "client.set_question(question)\n",
    "\n",
    "DEFAULT_ROOT = r\"../dof_markdown\"\n",
    "\n",
    "file_util = FileUtil(root_directory=DEFAULT_ROOT, client=client)\n",
    "\n",
    "# Create and display the interface\n",
    "controls = create_processing_interface(client, file_util)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
