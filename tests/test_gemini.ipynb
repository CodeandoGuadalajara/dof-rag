{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Procesamiento de Imágenes con Gemini API\n",
    "Esta libreta de Jupyter Notebook contiene el código dividido en celdas para facilitar su ejecución y comprensión. El script utiliza la API de Gemini para procesar imágenes y textos, registrando resultados y utilizando concurrencia para mejorar el rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"python-dotenv>=0.9.9\",\n",
    "#   \"google>=0.3.0\",\n",
    "#   \"google-genai>=1.3.0\",\n",
    "#   \"pillow\"\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import re\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Creamos un lock global para sincronizar el acceso a la API\n",
    "api_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición de la clase GeminiConfig y funciones de inicialización\n",
    "\n",
    "class GeminiConfig:\n",
    "    \"\"\"\n",
    "    Clase para manejar la configuración de la API de Gemini.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                model=\"gemini-2.0-flash\", \n",
    "                max_tokens=256, \n",
    "                temperature=1.0,\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                response_mime_type=\"text/plain\"):\n",
    "        # Obtener la API key desde variables de entorno\n",
    "        self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"No se encontró la variable de entorno GEMINI_API_KEY. Por favor, configúrela.\")\n",
    "            \n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # Parámetros de generación\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.top_k = top_k\n",
    "        self.response_mime_type = response_mime_type\n",
    "        \n",
    "    def get_generate_config(self):\n",
    "        \"\"\"Retorna la configuración para la generación de contenido\"\"\"\n",
    "        return types.GenerateContentConfig(\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            top_k=self.top_k,\n",
    "            max_output_tokens=self.max_tokens,\n",
    "            response_mime_type=self.response_mime_type,\n",
    "        )\n",
    "        \n",
    "    def get_client(self):\n",
    "        \"\"\"Retorna un cliente inicializado con la API key configurada\"\"\"\n",
    "        return genai.Client(api_key=self.api_key)\n",
    "        \n",
    "    def get_output_filename(self):\n",
    "        \"\"\"Genera el nombre del archivo de salida basado en el modelo y tokens\"\"\"\n",
    "\n",
    "        # Usar el nombre del modelo completo, reemplazando caracteres problemáticos para nombres de archivo\n",
    "        model_name = self.model.replace(\".\", \"-\").replace(\":\", \"_\")\n",
    "        return f\"./tests/results/{model_name}_tokens_{self.max_tokens}.txt\"\n",
    "\n",
    "# Variable para el archivo de resultados - ahora será generada automáticamente\n",
    "def initialize_config(token_limit=256, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"Inicializa la configuración con el límite de tokens especificado\"\"\"\n",
    "    return GeminiConfig(max_tokens=token_limit, model=model)\n",
    "\n",
    "# Inicializar la configuración por defecto\n",
    "GEMINI_CONFIG = initialize_config()\n",
    "GEMINI_MODEL = GEMINI_CONFIG.model\n",
    "MAX_TOKENS = GEMINI_CONFIG.max_tokens\n",
    "archivo = GEMINI_CONFIG.get_output_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones auxiliares\n",
    "\n",
    "def estimar_tokens(texto):\n",
    "    \"\"\"\n",
    "    Estima el número de tokens en un texto.\n",
    "    Esta es una estimación aproximada, ya que la tokenización exacta \n",
    "    depende del tokenizador específico que usa Gemini.\n",
    "    \"\"\"\n",
    "    if not texto:\n",
    "        return 0\n",
    "        \n",
    "    # Método simple basado en palabras (aproximado)\n",
    "    palabras = re.findall(r'\\w+|[^\\w\\s]', texto)\n",
    "    num_palabras = len(palabras)\n",
    "    \n",
    "    # Algunos tokens son subpalabras, otros son múltiples palabras\n",
    "    # Factor de ajuste aproximado: 1.3 tokens por palabra\n",
    "    return round(num_palabras * 1.3)\n",
    "\n",
    "def initialize_gemini_client():\n",
    "    \"\"\"\n",
    "    Inicializa y retorna un cliente de Gemini.\n",
    "    \"\"\"\n",
    "    return GEMINI_CONFIG.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de procesamiento de imágenes y textos\n",
    "\n",
    "\n",
    "\n",
    "def process_image_with_gemini(client, image_path, question, stream=False):\n",
    "    \"\"\"\n",
    "    Procesa una única imagen usando Gemini.\n",
    "    Se protege cada llamada a la API con un lock para evitar problemas de concurrencia.\n",
    "    Retorna un diccionario con los tiempos y las respuestas, o un error en caso de fallo.\n",
    "    \n",
    "    Si stream=True, usa el modo streaming para la generación de respuestas.\n",
    "    \"\"\"\n",
    "    result = {\"image_path\": image_path}\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error abriendo imagen: {str(e)}\"\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        with api_lock:\n",
    "            # Generar descripción de la imagen\n",
    "            caption_start = time.time()\n",
    "            \n",
    "            if not stream:\n",
    "                caption_response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[\"Describe esta imagen de manera breve y concisa. Limita tu respuesta a máximo 150 palabras. Responde COMPLETAMENTE en español.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                caption = caption_response.text if hasattr(caption_response, 'text') else str(caption_response)\n",
    "            else:\n",
    "                # Modo streaming para la descripción\n",
    "                caption = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[\"Describe esta imagen de manera breve y concisa. Limita tu respuesta a máximo 150 palabras. Responde COMPLETAMENTE en español.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    caption += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "            \n",
    "            caption_time = time.time() - caption_start\n",
    "            caption_tokens = estimar_tokens(caption)\n",
    "\n",
    "            # Responder a la pregunta sobre la imagen\n",
    "            query_start = time.time()\n",
    "            \n",
    "            if not stream:\n",
    "                answer_response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[question + \" Sé muy breve y conciso. Responde COMPLETAMENTE en español. Máximo 100 palabras.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                answer = answer_response.text if hasattr(answer_response, 'text') else str(answer_response)\n",
    "            else:\n",
    "                # Modo streaming para la respuesta\n",
    "                answer = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[question + \" Sé muy breve y conciso. Responde COMPLETAMENTE en español. Máximo 100 palabras.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    answer += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "            \n",
    "            query_time = time.time() - query_start\n",
    "            answer_tokens = estimar_tokens(answer)\n",
    "        \n",
    "        total_time = time.time() - start\n",
    "        result[\"caption_time\"] = caption_time\n",
    "        result[\"query_time\"] = query_time\n",
    "        result[\"total_time\"] = total_time\n",
    "        result[\"caption\"] = caption\n",
    "        result[\"answer\"] = answer\n",
    "        result[\"caption_tokens\"] = caption_tokens\n",
    "        result[\"answer_tokens\"] = answer_tokens\n",
    "        result[\"total_tokens\"] = caption_tokens + answer_tokens\n",
    "        result[\"streaming\"] = stream\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error procesando imagen con Gemini: {str(e)}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_with_gemini(client, prompt, stream=False):\n",
    "    \"\"\"\n",
    "    Procesa un texto usando Gemini.\n",
    "    \"\"\"\n",
    "    result = {\"prompt\": prompt}\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        with api_lock:\n",
    "            if not stream:\n",
    "                response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=prompt,\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                text_response = response.text if hasattr(response, 'text') else str(response)\n",
    "            else:\n",
    "                # Modo streaming\n",
    "                text_response = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=prompt,\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    text_response += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "                    # Si se desea ver la respuesta en tiempo real, descomentar:\n",
    "                    # print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "        process_time = time.time() - start\n",
    "        tokens = estimar_tokens(text_response)\n",
    "        \n",
    "        result[\"response\"] = text_response\n",
    "        result[\"process_time\"] = process_time\n",
    "        result[\"tokens\"] = tokens\n",
    "        result[\"streaming\"] = stream\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error procesando texto con Gemini: {str(e)}\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones para guardar resultados\n",
    "\n",
    "\n",
    "def guardar_resultado_imagen(img_res, log_file=None):\n",
    "    \"\"\"\n",
    "    Guarda el resultado de una imagen en el archivo de resultados (modo append).\n",
    "    \"\"\"\n",
    "    if log_file is None:\n",
    "        log_file = GEMINI_CONFIG.get_output_filename()\n",
    "        \n",
    "    separator = \"\\n\" + \"=\" * 80 + \"\\n\"\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(separator)\n",
    "        f.write(\"RESULTADOS DE IMAGEN\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Modelo: {GEMINI_MODEL}, Tokens máx: {MAX_TOKENS}\\n\")\n",
    "        f.write(f\"Imagen: {os.path.basename(img_res.get('image_path', 'N/A'))}\\n\")\n",
    "        \n",
    "        if \"error\" in img_res:\n",
    "            f.write(\"Error: \" + img_res[\"error\"] + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\nMÉTRICAS:\\n\")\n",
    "            f.write(f\"- Descripción: {img_res.get('caption_time', 0):.4f}s | \")\n",
    "            f.write(f\"Query: {img_res.get('query_time', 0):.4f}s | \")\n",
    "            f.write(f\"Total: {img_res.get('total_time', 0):.4f}s | \")\n",
    "            f.write(f\"Tokens: {img_res.get('total_tokens', 0)}\\n\")\n",
    "            \n",
    "            f.write(\"\\nRESULTADOS:\\n\")\n",
    "            f.write(\"Pregunta: ¿Qué se observa en esta imagen?\\n\\n\")\n",
    "            f.write(f\"Descripción: {img_res.get('caption', '')}\\n\\n\")\n",
    "            f.write(f\"Respuesta: {img_res.get('answer', '')}\\n\")\n",
    "        f.write(separator + \"\\n\")\n",
    "\n",
    "def guardar_resumen_final(results, log_file=None):\n",
    "    \"\"\"\n",
    "    Guarda un resumen final de todos los resultados al final del archivo.\n",
    "    \"\"\"\n",
    "    if log_file is None:\n",
    "        log_file = GEMINI_CONFIG.get_output_filename()\n",
    "        \n",
    "    separator = \"\\n\" + \"=\" * 80 + \"\\n\"\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(separator)\n",
    "        f.write(\"RESUMEN FINAL DE PROCESAMIENTO\\n\")\n",
    "        f.write(f\"Timestamp finalización: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Modelo: {GEMINI_MODEL}, Tokens máx: {MAX_TOKENS}\\n\")\n",
    "        \n",
    "        # Estadísticas básicas\n",
    "        total_images = len(results.get(\"images\", []))\n",
    "        successful_images = sum(1 for img in results.get(\"images\", []) if \"error\" not in img)\n",
    "        failed_images = total_images - successful_images\n",
    "        \n",
    "        f.write(f\"Total imágenes procesadas: {total_images}\\n\")\n",
    "        f.write(f\"Exitosas: {successful_images}, Errores: {failed_images}\\n\")\n",
    "        \n",
    "        # Tiempo total\n",
    "        total_time = results.get(\"total_time\", 0)\n",
    "        f.write(f\"Tiempo total de procesamiento: {total_time:.2f} segundos\\n\")\n",
    "        \n",
    "        if successful_images > 0:\n",
    "            # Calcular promedios\n",
    "            avg_caption_time = sum(img.get(\"caption_time\", 0) for img in results.get(\"images\", []) \n",
    "                                if \"error\" not in img) / successful_images\n",
    "            avg_query_time = sum(img.get(\"query_time\", 0) for img in results.get(\"images\", []) \n",
    "                              if \"error\" not in img) / successful_images\n",
    "            avg_total_time = sum(img.get(\"total_time\", 0) for img in results.get(\"images\", []) \n",
    "                             if \"error\" not in img) / successful_images\n",
    "            avg_tokens = sum(img.get(\"total_tokens\", 0) for img in results.get(\"images\", []) \n",
    "                         if \"error\" not in img) / successful_images\n",
    "            \n",
    "            f.write(\"\\nTiempos promedio por imagen:\\n\")\n",
    "            f.write(f\"- Descripción: {avg_caption_time:.4f}s | \")\n",
    "            f.write(f\"Query: {avg_query_time:.4f}s | \")\n",
    "            f.write(f\"Total: {avg_total_time:.4f}s | \")\n",
    "            f.write(f\"Tokens: {avg_tokens:.1f}\\n\")\n",
    "        \n",
    "        f.write(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar imágenes en paralelo utilizando hilos\n",
    "\n",
    "def process_images_with_threads(image_files, question, max_workers=None, use_streaming=False):\n",
    "    \"\"\"\n",
    "    Procesa las imágenes en hilos usando ThreadPoolExecutor y la API de Gemini.\n",
    "    \n",
    "    Parámetros:\n",
    "    - image_files: Lista de rutas de imágenes a procesar\n",
    "    - question: Pregunta a realizar sobre cada imagen\n",
    "    - max_workers: Número máximo de workers en el ThreadPool\n",
    "    - use_streaming: Si es True, usa el modo streaming para las respuestas\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results[\"model\"] = GEMINI_CONFIG.model\n",
    "    results[\"max_tokens\"] = GEMINI_CONFIG.max_tokens\n",
    "    start_total = time.time()\n",
    "    \n",
    "    try:\n",
    "        init_start = time.time()\n",
    "        client = initialize_gemini_client()\n",
    "        init_time = time.time() - init_start\n",
    "        results[\"init_time\"] = init_time\n",
    "    except Exception as e:\n",
    "        results[\"error\"] = f\"Error inicializando el cliente de Gemini: {str(e)}\"\n",
    "        return results\n",
    "\n",
    "    results[\"images\"] = []\n",
    "    total_images = len(image_files)\n",
    "    completed = 0\n",
    "\n",
    "    # Calcular el 70% de los núcleos disponibles\n",
    "    if max_workers is None:\n",
    "        num_cpu = os.cpu_count() or 1\n",
    "        max_workers = max(1, int(num_cpu * 0.7))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_image_with_gemini, client, img_path, question, use_streaming): img_path for img_path in image_files}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            image_result = future.result()\n",
    "            results[\"images\"].append(image_result)\n",
    "            completed += 1\n",
    "\n",
    "            # Guardar inmediatamente el resultado de la imagen en el archivo\n",
    "            guardar_resultado_imagen(image_result)\n",
    "\n",
    "            elapsed = time.time() - start_total\n",
    "            avg_time = elapsed / completed\n",
    "            remaining = total_images - completed\n",
    "            estimated_remaining = remaining * avg_time\n",
    "            print(f\"[Gemini] Imagen {completed}/{total_images} procesada en {image_result.get('total_time', 0):.2f} s. \"\n",
    "                  f\"Tiempo total: {elapsed:.2f} s. Estimado restante: {estimated_remaining:.2f} s\", flush=True)\n",
    "    \n",
    "    results[\"total_time\"] = time.time() - start_total\n",
    "    \n",
    "    # Guardar resumen final\n",
    "    guardar_resumen_final(results)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros configurados manualmente\n",
    "tokens = 256  # Número máximo de tokens (puede cambiarse a 512, 1024, etc.)\n",
    "streaming = False  # Cambia a True para activar el modo streaming\n",
    "image_directory = './imagenes_prueba'  # Directorio con las imágenes de prueba\n",
    "question = '¿Qué se observa en esta imagen?'  # Pregunta para cada imagen\n",
    "modelo = \"gemini-2.0-flash\"  # Modelo a utilizar, por defecto \"gemini-2.0-flash\"\n",
    "\n",
    "# Inicializar configuración de Gemini\n",
    "GEMINI_CONFIG = initialize_config(tokens, modelo)\n",
    "GEMINI_MODEL = GEMINI_CONFIG.model\n",
    "MAX_TOKENS = GEMINI_CONFIG.max_tokens\n",
    "archivo = GEMINI_CONFIG.get_output_filename()\n",
    "\n",
    "print(f'Resultados de predicciones con Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buscar imágenes en el directorio indicado\n",
    "image_files = glob.glob(os.path.join(image_directory, '*.*'))\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')\n",
    "image_files = [img for img in image_files if img.lower().endswith(valid_extensions)]\n",
    "\n",
    "if not image_files:\n",
    "    print('No se encontraron imágenes en el directorio:', image_directory)\n",
    "else:\n",
    "    print('\\n' + '=' * 80)\n",
    "    print(f'Iniciando predicciones con Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}')\n",
    "    print(f'Modo streaming: {\"Activado\" if streaming else \"Desactivado\"}')\n",
    "    print(f'Archivo de resultados: {archivo}')\n",
    "\n",
    "    # Escribir encabezado de resultados\n",
    "    with open(archivo, 'w', encoding='utf-8') as f:\n",
    "        f.write(f'Resultados de predicciones con Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}\\n')\n",
    "\n",
    "    # Procesar imágenes (se asume que process_images_with_threads está definida)\n",
    "    results = process_images_with_threads(image_files, question, use_streaming=streaming)\n",
    "\n",
    "    if 'error' in results:\n",
    "        print(f'Error: {results[\"error\"]}')\n",
    "    else:\n",
    "        print(f'Procesamiento finalizado en {results.get(\"total_time\", 0):.2f} s')\n",
    "        print(f'Resumen guardado en: {archivo}')\n",
    "\n",
    "    print('=' * 80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(archivo, 'r', encoding='utf-8') as file:\n",
    "    contenido = file.read()\n",
    "    print(contenido)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
