{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Image Processing with Gemini API\n",
    "This Jupyter Notebook contains code divided into cells to facilitate its execution and understanding. The script uses the Gemini API to process images and texts, recording results and using concurrency to improve performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# /// script\n",
    "# dependencies = [\n",
    "#   \"python-dotenv>=0.9.9\",\n",
    "#   \"google>=0.3.0\",\n",
    "#   \"google-genai>=1.3.0\",\n",
    "#   \"pillow\"\n",
    "# ]\n",
    "# ///\n",
    "\n",
    "from PIL import Image\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "import glob\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Create a global lock to synchronize API access\n",
    "api_lock = threading.Lock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of GeminiConfig class and initialization functions\n",
    "\n",
    "class GeminiConfig:\n",
    "    \"\"\"\n",
    "    Class to handle Gemini API configuration.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                model=\"gemini-2.0-flash\", \n",
    "                max_tokens=256, \n",
    "                temperature=1.0,\n",
    "                top_p=0.95,\n",
    "                top_k=40,\n",
    "                response_mime_type=\"text/plain\"):\n",
    "        # Get API key from environment variables\n",
    "        self.api_key = os.environ.get(\"GEMINI_API_KEY\")\n",
    "        if not self.api_key:\n",
    "            raise ValueError(\"GEMINI_API_KEY environment variable not found. Please configure it.\")\n",
    "            \n",
    "        self.model = model\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "        # Generation parameters\n",
    "        self.temperature = temperature\n",
    "        self.top_p = top_p\n",
    "        self.top_k = top_k\n",
    "        self.response_mime_type = response_mime_type\n",
    "        \n",
    "    def get_generate_config(self):\n",
    "        \"\"\"Returns the configuration for content generation\"\"\"\n",
    "        return types.GenerateContentConfig(\n",
    "            temperature=self.temperature,\n",
    "            top_p=self.top_p,\n",
    "            top_k=self.top_k,\n",
    "            max_output_tokens=self.max_tokens,\n",
    "            response_mime_type=self.response_mime_type,\n",
    "        )\n",
    "        \n",
    "    def get_client(self):\n",
    "        \"\"\"Returns a client initialized with the configured API key\"\"\"\n",
    "        return genai.Client(api_key=self.api_key)\n",
    "        \n",
    "    def get_output_filename(self):\n",
    "        \"\"\"Generates the output filename based on model and tokens\"\"\"\n",
    "\n",
    "        # Use the full model name, replacing problematic characters for filenames\n",
    "        model_name = self.model.replace(\".\", \"-\").replace(\":\", \"_\")\n",
    "        return f\"./tests/results/{model_name}_tokens_{self.max_tokens}.txt\"\n",
    "\n",
    "# Variable for results file - will now be generated automatically\n",
    "def initialize_config(token_limit=256, model=\"gemini-2.0-flash\"):\n",
    "    \"\"\"Initializes configuration with specified token limit\"\"\"\n",
    "    return GeminiConfig(max_tokens=token_limit, model=model)\n",
    "\n",
    "# Initialize default configuration\n",
    "GEMINI_CONFIG = initialize_config()\n",
    "GEMINI_MODEL = GEMINI_CONFIG.model\n",
    "MAX_TOKENS = GEMINI_CONFIG.max_tokens\n",
    "output_file = GEMINI_CONFIG.get_output_filename()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def estimate_tokens(text):\n",
    "    \"\"\"\n",
    "    Estimates the number of tokens in a text.\n",
    "    This is an approximate estimation, as exact tokenization \n",
    "    depends on the specific tokenizer used by Gemini.\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return 0\n",
    "        \n",
    "    # Simple method based on words (approximate)\n",
    "    words = re.findall(r'\\w+|[^\\w\\s]', text)\n",
    "    num_words = len(words)\n",
    "    \n",
    "    # Some tokens are subwords, others are multiple words\n",
    "    # Approximate adjustment factor: 1.3 tokens per word\n",
    "    return round(num_words * 1.3)\n",
    "\n",
    "def initialize_gemini_client():\n",
    "    \"\"\"\n",
    "    Initializes and returns a Gemini client.\n",
    "    \"\"\"\n",
    "    return GEMINI_CONFIG.get_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image and text processing functions\n",
    "\n",
    "\n",
    "\n",
    "def process_image_with_gemini(client, image_path, question, stream=False):\n",
    "    \"\"\"\n",
    "    Processes a single image using Gemini.\n",
    "    Each API call is protected with a lock to avoid concurrency issues.\n",
    "    Returns a dictionary with times and responses, or an error in case of failure.\n",
    "    \n",
    "    If stream=True, uses streaming mode for response generation.\n",
    "    \"\"\"\n",
    "    result = {\"image_path\": image_path}\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        image = Image.open(image_path)\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error opening image: {str(e)}\"\n",
    "        return result\n",
    "    \n",
    "    try:\n",
    "        with api_lock:\n",
    "            # Generate image description\n",
    "            caption_start = time.time()\n",
    "            \n",
    "            if not stream:\n",
    "                caption_response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[\"Describe esta imagen de manera breve y concisa. Limita tu respuesta a máximo 150 palabras. Responde COMPLETAMENTE en español.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                caption = caption_response.text if hasattr(caption_response, 'text') else str(caption_response)\n",
    "            else:\n",
    "                # Streaming mode for description\n",
    "                caption = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[\"Describe esta imagen de manera breve y concisa. Limita tu respuesta a máximo 150 palabras. Responde COMPLETAMENTE en español.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    caption += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "            \n",
    "            caption_time = time.time() - caption_start\n",
    "            caption_tokens = estimate_tokens(caption)\n",
    "\n",
    "            # Answer question about the image\n",
    "            query_start = time.time()\n",
    "            \n",
    "            if not stream:\n",
    "                answer_response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[question + \" Sé muy breve y conciso. Responde COMPLETAMENTE en español. Máximo 100 palabras.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                answer = answer_response.text if hasattr(answer_response, 'text') else str(answer_response)\n",
    "            else:\n",
    "                # Streaming mode for answer\n",
    "                answer = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=[question + \" Sé muy breve y conciso. Responde COMPLETAMENTE en español. Máximo 100 palabras.\", image],\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    answer += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "            \n",
    "            query_time = time.time() - query_start\n",
    "            answer_tokens = estimate_tokens(answer)\n",
    "        \n",
    "        total_time = time.time() - start\n",
    "        result[\"caption_time\"] = caption_time\n",
    "        result[\"query_time\"] = query_time\n",
    "        result[\"total_time\"] = total_time\n",
    "        result[\"caption\"] = caption\n",
    "        result[\"answer\"] = answer\n",
    "        result[\"caption_tokens\"] = caption_tokens\n",
    "        result[\"answer_tokens\"] = answer_tokens\n",
    "        result[\"total_tokens\"] = caption_tokens + answer_tokens\n",
    "        result[\"streaming\"] = stream\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error processing image with Gemini: {str(e)}\"\n",
    "    \n",
    "    return result\n",
    "\n",
    "def process_text_with_gemini(client, prompt, stream=False):\n",
    "    \"\"\"\n",
    "    Processes text using Gemini.\n",
    "    \"\"\"\n",
    "    result = {\"prompt\": prompt}\n",
    "    start = time.time()\n",
    "    \n",
    "    try:\n",
    "        with api_lock:\n",
    "            if not stream:\n",
    "                response = client.models.generate_content(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=prompt,\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                )\n",
    "                text_response = response.text if hasattr(response, 'text') else str(response)\n",
    "            else:\n",
    "                # Streaming mode\n",
    "                text_response = \"\"\n",
    "                for chunk in client.models.generate_content_stream(\n",
    "                    model=GEMINI_CONFIG.model,\n",
    "                    contents=prompt,\n",
    "                    config=GEMINI_CONFIG.get_generate_config()\n",
    "                ):\n",
    "                    text_response += chunk.text if hasattr(chunk, 'text') else \"\"\n",
    "                    # If you want to see the response in real-time, uncomment:\n",
    "                    # print(chunk.text, end=\"\", flush=True)\n",
    "            \n",
    "        process_time = time.time() - start\n",
    "        tokens = estimate_tokens(text_response)\n",
    "        \n",
    "        result[\"response\"] = text_response\n",
    "        result[\"process_time\"] = process_time\n",
    "        result[\"tokens\"] = tokens\n",
    "        result[\"streaming\"] = stream\n",
    "    except Exception as e:\n",
    "        result[\"error\"] = f\"Error processing text with Gemini: {str(e)}\"\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to save results\n",
    "\n",
    "\n",
    "def save_image_result(img_res, log_file=None):\n",
    "    \"\"\"\n",
    "    Saves the result of an image to the results file (append mode).\n",
    "    \"\"\"\n",
    "    if log_file is None:\n",
    "        log_file = GEMINI_CONFIG.get_output_filename()\n",
    "        \n",
    "    separator = \"\\n\" + \"=\" * 80 + \"\\n\"\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(separator)\n",
    "        f.write(\"IMAGE RESULTS\\n\")\n",
    "        f.write(f\"Timestamp: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Model: {GEMINI_MODEL}, Max tokens: {MAX_TOKENS}\\n\")\n",
    "        f.write(f\"Image: {os.path.basename(img_res.get('image_path', 'N/A'))}\\n\")\n",
    "        \n",
    "        if \"error\" in img_res:\n",
    "            f.write(\"Error: \" + img_res[\"error\"] + \"\\n\")\n",
    "        else:\n",
    "            f.write(\"\\nMETRICS:\\n\")\n",
    "            f.write(f\"- Description: {img_res.get('caption_time', 0):.4f}s | \")\n",
    "            f.write(f\"Query: {img_res.get('query_time', 0):.4f}s | \")\n",
    "            f.write(f\"Total: {img_res.get('total_time', 0):.4f}s | \")\n",
    "            f.write(f\"Tokens: {img_res.get('total_tokens', 0)}\\n\")\n",
    "            \n",
    "            f.write(\"\\nRESULTS:\\n\")\n",
    "            f.write(\"Question: What can be observed in this image?\\n\\n\")\n",
    "            f.write(f\"Description: {img_res.get('caption', '')}\\n\\n\")\n",
    "            f.write(f\"Answer: {img_res.get('answer', '')}\\n\")\n",
    "        f.write(separator + \"\\n\")\n",
    "\n",
    "def save_final_summary(results, log_file=None):\n",
    "    \"\"\"\n",
    "    Saves a final summary of all results at the end of the file.\n",
    "    \"\"\"\n",
    "    if log_file is None:\n",
    "        log_file = GEMINI_CONFIG.get_output_filename()\n",
    "        \n",
    "    separator = \"\\n\" + \"=\" * 80 + \"\\n\"\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(separator)\n",
    "        f.write(\"FINAL PROCESSING SUMMARY\\n\")\n",
    "        f.write(f\"Completion timestamp: {datetime.datetime.now().isoformat()}\\n\")\n",
    "        f.write(f\"Model: {GEMINI_MODEL}, Max tokens: {MAX_TOKENS}\\n\")\n",
    "        \n",
    "        # Basic statistics\n",
    "        total_images = len(results.get(\"images\", []))\n",
    "        successful_images = sum(1 for img in results.get(\"images\", []) if \"error\" not in img)\n",
    "        failed_images = total_images - successful_images\n",
    "        \n",
    "        f.write(f\"Total images processed: {total_images}\\n\")\n",
    "        f.write(f\"Successful: {successful_images}, Errors: {failed_images}\\n\")\n",
    "        \n",
    "        # Total time\n",
    "        total_time = results.get(\"total_time\", 0)\n",
    "        f.write(f\"Total processing time: {total_time:.2f} seconds\\n\")\n",
    "        \n",
    "        if successful_images > 0:\n",
    "            # Calculate averages\n",
    "            avg_caption_time = sum(img.get(\"caption_time\", 0) for img in results.get(\"images\", []) \n",
    "                                if \"error\" not in img) / successful_images\n",
    "            avg_query_time = sum(img.get(\"query_time\", 0) for img in results.get(\"images\", []) \n",
    "                              if \"error\" not in img) / successful_images\n",
    "            avg_total_time = sum(img.get(\"total_time\", 0) for img in results.get(\"images\", []) \n",
    "                             if \"error\" not in img) / successful_images\n",
    "            avg_tokens = sum(img.get(\"total_tokens\", 0) for img in results.get(\"images\", []) \n",
    "                         if \"error\" not in img) / successful_images\n",
    "            \n",
    "            f.write(\"\\nAverage time per image:\\n\")\n",
    "            f.write(f\"- Description: {avg_caption_time:.4f}s | \")\n",
    "            f.write(f\"Query: {avg_query_time:.4f}s | \")\n",
    "            f.write(f\"Total: {avg_total_time:.4f}s | \")\n",
    "            f.write(f\"Tokens: {avg_tokens:.1f}\\n\")\n",
    "        \n",
    "        f.write(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process images in parallel using threads\n",
    "\n",
    "def process_images_with_threads(image_files, question, max_workers=None, use_streaming=False):\n",
    "    \"\"\"\n",
    "    Processes images in threads using ThreadPoolExecutor and the Gemini API.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_files: List of image paths to process\n",
    "    - question: Question to ask about each image\n",
    "    - max_workers: Maximum number of workers in the ThreadPool\n",
    "    - use_streaming: If True, uses streaming mode for responses\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    results[\"model\"] = GEMINI_CONFIG.model\n",
    "    results[\"max_tokens\"] = GEMINI_CONFIG.max_tokens\n",
    "    start_total = time.time()\n",
    "    \n",
    "    try:\n",
    "        init_start = time.time()\n",
    "        client = initialize_gemini_client()\n",
    "        init_time = time.time() - init_start\n",
    "        results[\"init_time\"] = init_time\n",
    "    except Exception as e:\n",
    "        results[\"error\"] = f\"Error initializing Gemini client: {str(e)}\"\n",
    "        return results\n",
    "\n",
    "    results[\"images\"] = []\n",
    "    total_images = len(image_files)\n",
    "    completed = 0\n",
    "\n",
    "    # Calculate 70% of available cores\n",
    "    if max_workers is None:\n",
    "        num_cpu = os.cpu_count() or 1\n",
    "        max_workers = max(1, int(num_cpu * 0.7))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_image_with_gemini, client, img_path, question, use_streaming): img_path for img_path in image_files}\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            image_result = future.result()\n",
    "            results[\"images\"].append(image_result)\n",
    "            completed += 1\n",
    "\n",
    "            # Immediately save the image result to the file\n",
    "            save_image_result(image_result)\n",
    "\n",
    "            elapsed = time.time() - start_total\n",
    "            avg_time = elapsed / completed\n",
    "            remaining = total_images - completed\n",
    "            estimated_remaining = remaining * avg_time\n",
    "            print(f\"[Gemini] Image {completed}/{total_images} processed in {image_result.get('total_time', 0):.2f} s. \"\n",
    "                  f\"Total time: {elapsed:.2f} s. Estimated remaining: {estimated_remaining:.2f} s\", flush=True)\n",
    "    \n",
    "    results[\"total_time\"] = time.time() - start_total\n",
    "    \n",
    "    # Save final summary\n",
    "    save_final_summary(results)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually configured parameters\n",
    "tokens = 256  # Maximum number of tokens (can be changed to 512, 1024, etc.)\n",
    "streaming = False  # Change to True to activate streaming mode\n",
    "image_directory = './imagenes_prueba'  # Directory with test images\n",
    "question = '¿Qué se observa en esta imagen?'  # Question for each image\n",
    "model = \"gemini-2.0-flash\"  # Model to use, default \"gemini-2.0-flash\"\n",
    "\n",
    "# Initialize Gemini configuration\n",
    "GEMINI_CONFIG = initialize_config(tokens, model)\n",
    "GEMINI_MODEL = GEMINI_CONFIG.model\n",
    "MAX_TOKENS = GEMINI_CONFIG.max_tokens\n",
    "output_file = GEMINI_CONFIG.get_output_filename()\n",
    "\n",
    "print(f'Prediction results with Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find images in the specified directory\n",
    "image_files = glob.glob(os.path.join(image_directory, '*.*'))\n",
    "valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff')\n",
    "image_files = [img for img in image_files if img.lower().endswith(valid_extensions)]\n",
    "\n",
    "if not image_files:\n",
    "    print('No images found in directory:', image_directory)\n",
    "else:\n",
    "    print('\\n' + '=' * 80)\n",
    "    print(f'Starting predictions with Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}')\n",
    "    print(f'Streaming mode: {\"Enabled\" if streaming else \"Disabled\"}')\n",
    "    print(f'Results file: {output_file}')\n",
    "\n",
    "    # Write results header\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(f'Prediction results with Gemini ({GEMINI_MODEL}) - max_tokens: {MAX_TOKENS}\\n')\n",
    "\n",
    "    # Process images (assumes process_images_with_threads is defined)\n",
    "    results = process_images_with_threads(image_files, question, use_streaming=streaming)\n",
    "\n",
    "    if 'error' in results:\n",
    "        print(f'Error: {results[\"error\"]}')\n",
    "    else:\n",
    "        print(f'Processing completed in {results.get(\"total_time\", 0):.2f} s')\n",
    "        print(f'Summary saved in: {output_file}')\n",
    "\n",
    "    print('=' * 80 + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "    print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
